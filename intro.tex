\chapter{The evolution  \statusgreen}\label{chap:vision}



The rest of this chapter gives background information to the evolution leading to Swarm and finally the vision of Swarm. \ref{sec:historical_context} lays out a historical analysis of the World Wide Web, focusing on how it became the place it is today.
\ref{sec:fair-data} explains the importance of data sovereignty, collective information and \gloss{fair data economy}. It discusses why a self-sovereign society needs infrastructure to collectively host, move and process data.
Finally, \ref{sec:vision} recaps the values behind the vision, spells out the requirements of the technology and lays down the design principles that guide us in manifesting Swarm.

\section{Historical context  \statusgreen}\label{sec:historical_context}
\green{}
While the Internet in general and the \gloss{World Wide Web} (\gloss{WWW}) in particular dramatically reduced the costs of disseminating information, putting a publisher's power at (almost) every user's fingertips, these costs are still not zero and their allocation heavily influences who gets to publish what content and who gets to consume it.

In order to appreciate the problems we are trying to solve, a little history of the evolution of the \gloss{World Wide Web} is useful.

\subsection{Web 1.0 \statusgreen}\label{sec:web_1}

In the times of \gloss{Web 1.0}, in order to have your content accessible to the whole world, you would typically fire up a web server or use some web hosting (free or cheap) and upload your content that could be navigated through a set of HTML pages. If your content was unpopular, you still had to either maintain the server or pay the hosting to keep it accessible, but true disaster struck when, for one reason or another, it became popular (e.g. you got "Slashdotted"). At this point, your traffic bill skyrocketed just before either your server crashed under the load or your hosting provider throttled your bandwidth to the point of making your content essentially unavailable for the majority of your audience. If you wanted to stay popular, you had to invest in high availability clusters connected with fat pipes; your costs grew together with your popularity, without any obvious way to cover them. There were very few practical ways to allow (let alone require) your audience to share the ensuing financial burden directly.

The common wisdom at the time was that it would be the \gloss{ISP} that would come to the rescue, since in the early days of the Web revolution bargaining about peering arrangements between ISP's involved arguments about where providers and consumers are located, and which ISP is making money from the other's network. Indeed, when there was a sharp imbalance between originators of TCP connection requests (aka SYN packets), it was customary for the originator ISP to pay the recipient ISP, which made the latter somewhat incentivised to help support those hosting popular content. In practice, however, this incentive structure usually resulted in putting a free \emph{pr0n} or \emph{warez} server in the server room to tilt back the scales of SYN packet counters. Blogs catering to a niche audience had no way of competing and were generally left out in the cold. Note, however, that back then, creator-publishers still typically owned their content.

\subsection{Web 2.0 \statusgreen}\label{sec:web_2}

The transition to \gloss{Web 2.0} changed much of that. Context-sensitive targeted advertising offered a Faustian bargain to content producers. As in "We will give you scalable hosting that will cope with any traffic your audience throws at it, but in return you must give us substantial control over your content: we are going to track each member of your audience and collect -- and own -- as much of their personal data as we are able, we will decide who can and who cannot see it, we will proactively censor it, and we may willingly share your data with authorities where requested, even if only required for the same reason". As a consequence, millions of small content producers created immense value for very few corporations, getting only peanuts (typically, free hosting) in exchange.

\subsection{Peer-to-peer networks \statusgreen}\label{sec:peer_to_peer}

At the same time, however, the \gloss{peer-to-peer} (\gloss{P2P}) revolution was gathering pace. Actually, P2P traffic very soon took over the majority of packets flowing through the pipes, quickly overtaking the above mentioned SYN-bait servers. If anything, it proved beyond doubt that end-users, by using their hitherto massively underutilised upstream bandwidth, could achieve the same kind of availability and bandwidth for their content as previously provided only by big corporations with data centers attached to the fattest pipes of the Internet's backbone. What's more, this could be achieved at a fraction of the cost. In particular, users retained a lot more control and freedom over their data. Finally, this mode of data distribution proved to be remarkably resilient even in the face of powerful and well-funded entities exerting great efforts to shut it down.


On the other hand, even the most evolved mode of \gloss{P2P} file sharing, which is tracker-less Bittorrent \cite{pouwelse2005bittorrent}, is just that: file sharing. It is not suitable for providing the kind of interactive, responsive experience that people came to expect from web applications on \gloss{Web 2.0}. One can only get so far by simply sharing upstream bandwidth, hard-drive space, and a tiny amount of computing power, without proper accounting and indexing. However, if you add to the mix a few more emergent technologies, most importantly, the blockchain, you get what we believe deserves the \gloss{Web 3.0} moniker: a decentralised, censorship-resistant way of sharing and even collectively creating interactive content, while retaining full control over it. The price is surprisingly low and mostly consists of the resources supplied by the super-computer (by yesteryear's standards) that you already own or can rent cheaply.

\subsection{The economics of BitTorrent and its limits \statusgreen}

The genius of \gloss{BitTorrent} lies in its clever resource optimisation \cite{cohen2003incentives}: if many clients want to download the same content from you, give them different parts of it and let them swap the missing parts between one another in a tit-for-tat fashion. This way, the upstream bandwidth use of a user hosting the content (\gloss{seeder} in BitTorrent parlance) is roughly the same, no matter how many clients want to download it simultaneously. This solves the most painful issue of the ancient \gloss{HTTP} underpinning the \gloss{World Wide Web}.

Cheating (i.e. feeding your peers with garbage) is discouraged by the use of hierarchical piecewise hashing, whereby a package offered for download is identified by a single short hash, and any part can be cryptographically proven to be a specific part of the package without all the other parts, with a very small overhead. 

This beautifully simple approach has three main shortcomings \cite{locher2006free,piatek2007incentives}, all somewhat related:

\begin{itemize}
\item \emph{lack of economic incentives} -- 
There are no built-in incentives to seed downloaded content. In particular, one cannot exchange his upstream bandwidth provided by seeding one's content for the downstream bandwidth required for downloading some other content. Effectively, the upstream bandwidth provided by seeding content to another user is not directly rewarded in any way.
\item \emph{initial latency} -- 
 Typically, downloads start slowly and with some delay. Clients that are further ahead in downloading have much more to offer to and much less to demand from newcomers. This results in BitTorrent downloads starting as a trickle before turning into a full-blown torrent of bits. This severely limits the use of BitTorrent in interactive applications that require both fast responses and high bandwidth.
\item \emph{lack of content addressing} -- Small \glossplural{chunk} of data can only be shared in the context of the larger file that they are part of. We find peers sharing the content that we seek by querying the \gloss{distributed hash table} (\gloss{DHT}) for the said file. In other words, the advertising of the available content happens at the level of files, as opposed to the chunks of data they are composed of. This leads to inefficiencies as the same chunks of data often appear verbatim in multiple files. 
\item \emph{no incentive to keep sharing} --
Nodes are not rewarded for their sharing efforts (storage and bandwidth) once they have achieved their objective of getting the missing parts of a file from their peers.
\end{itemize}

\subsection{Towards Web 3.0 \statusgreen}\label{sec:towards-web3}

The straightforward approach of using \gloss{BitTorrent} as a distribution mechanism for web content has been successfully implemented by \gloss{ZeroNet} \cite{zeronet}. However, because of the aforementioned issues with BitTorrent, ZeroNet fails to provide the same kind of responsive experience that users of web services came to expect. BitTorrent is also currently testing the waters with blockchain incentivisation with help of their own token \cite{tron2018,bittorrent2019}.

In order to enable responsive \glossplural{distributed web application} (called \glossplural{dapp} in \gloss{Web 3.0} communities), the \gloss{InterPlanetary File System} (\gloss{IPFS}) \cite{ipfs2014} had to introduce a few major improvements over BitTorrent. The most immediately apparent novelty is the highly web-compatible URL-based retrieval. In addition, the directory (also organized as a \gloss{DHT}) has been vastly improved, making it possible to search for any part of any file (called \gloss{chunk}). It has also been made very flexible and pluggable in order to work with any kind of storage back-end, be it a laptop with intermittent WiFi, or a sophisticated high-availability cluster in a fiber-optic connected data center.

Because of the improved directory, it is in the interest of greedy downloaders to balance the load they impose on other nodes; unlike in the case of BitTorrent, they do not need to be forced to do so. The naive default behavior of IPFS nodes is to download what they want as fast as they can from those who provide it, while automatically caching, advertising and uploading upon request everything they come across. They use their downstream bandwidth to the maximum extent they can, while they do not throttle their upstream bandwidth beyond its physical limits. This, together with a few very powerful and well-connected nodes provided by the company behind IPFS, results in a very impressive performance even without any additional incentive module.


One measure by which IPFS aims to shield its users from legal liability is that, just like in the case of BitTorrent, there is no such thing as "pushing" anything onto an IPFS node. Sharing anything on IPFS simply means making it available on one's own node and known in the directory. However, naive consumers immediately replicate all the content they download and also make it available. Public \gloss{HTTP} gateways (most of which are run by the company behind IPFS) provide automatic replication for whatever content is being accessed through them.

The same problem with lack of incentives is apparent in various other projects such as ZeroNet or MaidSafe. Incentivization for distributed document storage is still a relatively new research field, especially in the context of the brand new blockchain technology. The Tor network has seen suggestions \cite{jansen2014onions,ghoshetal2014tor} but these schemes are largely academic, they are not built-in at the heart of the underlying system. Bitcoin has also been repurposed to drive other systems like Permacoin \cite{miller2014permacoin}, some use their own blockchain, such as Sia \cite{vorick2014sia} or Filecoin \cite{filecoin2014} for IPFS.

What is still missing from the above incentive systems, is the possibility to rent out large amounts of disk space to those willing to pay for it, irrespective of the popularity of their content; and conversely there is also a way to deploy your interactive dynamic content to be stored in the cloud, a feature we call \gloss{upload and disappear}.

The objective of any incentive system for \gloss{P2P} content distribution is to encourage cooperative behavior and discourage \gloss{freeriding}: the uncompensated depletion of limited resources. The \gloss{incentive strategy} outlined here aspires to satisfy the following constraints:

\begin{itemize}
    \item It is in the node's own interest, regardless of whether other nodes follow it.
    \item It must be expensive to expend the resources of other nodes.
    \item It does not impose unreasonable overhead.
    \item It plays nice with "naive" nodes.
    \item It rewards those that play nice, including those following this strategy.
\end{itemize}

In the context of Swarm, storage and bandwidth are the two most important limited resources and this is reflected in our incentives scheme. The incentives for bandwidth use are designed to achieve speedy and reliable data provision, while the storage incentives are designed to ensure long term data preservation, ideally solving the "\gloss{upload and disappear}" problem.

\section{Fair data economy  \statusgreen}\label{sec:fair-data}
\green{}

In the era of \gloss{Web 3.0}, the Internet is no longer just a niche where geeks play, but has become a large conduit for value creation and growing economic activity. Yet the data economy in it's current state could not be labelled as fair, as the distribution of the gains is under the control of those controlling the data - mostly companies siloing the data. In order to move towards the goal of a \gloss{fair data economy} many social, legal and technological issues will have to be tackled. We will next attempt to  describe some of the problems that are currently present and how they will be addressed by Swarm. 

\subsection{The current state of the data economy  \statusgreen} \label{sec:dataeconomy}

Digital mirror worlds already exist, that contain shadows of physical things and contain unimaginably large amounts of data \cite{MirrorWorlds2020Feb}. Yet more and more data will be synced to these parallel worlds, requiring new infrastructure and markets, creating new business opportunities. Relatively crude measures exist for measuring the size of the data economy, but for the USA, one figure puts the financial value of data (with related software and intellectual property) at \$1.4trn-2trn in 2019 \cite{MirrorWorlds2020Feb}. The EU Commission projects the figures for the data economy in the EU27 for 2025 at €829bln (up from €301bln in 2018) \cite{EUDataStrategy2020Feb}. 

The asymmetric distribution of wealth generated by the existing data economy has been put forward as a major humanitarian issue \cite{TheWinner2020Feb}. As efficiency and productivity will rise, on account of better data, the spoils of this will need to be distributed. Today, the spoils are distributed quite unequally. The larger the companies' data set (silo), the more it can learn from the data, attract more users and even more data. Currently, this is apparent with the large tech companies, but it is predicted that it will increasingly apply to non-tech companies and even nation states. Hence, companies are racing to become dominant in a particular sector. And countries hosting these platforms will gain an advantage. As Africa and Latin America host so few of these, they risk becoming providers of raw data and paying for the intelligence provided, as has been warned by the United Nations Conference on Trade and Development \cite{TheWinner2020Feb}. Another problem is that if a large company monopolizes a data market, it could also be the sole purchaser of "data labour" - maintaining complete control of setting prices for it and manipulating them to keep them artificially low. As a solution, citizens could organise into "data co-operatives", who would then act as trade unions do in conventional economy. 

Flows of data are becoming increasingly blocked and filtered by governments for reasons of protecting people, sovereignty and national economy \cite{VirtualNationalism2020Feb}. The leaks revealed by several security experts have shown the governments that data should be kept close to home and not left to reside in other countries. GDPR is one kind of "digital border" that has gone up - data may leave the EU only if appropriate safeguards are in place. Other countries, like India, Russia and China, have their own geographic limitations regarding data. The EU Commission has pledged to closely monitor the policies of these countries and address any limits or restrictions to data flows in trade negotiations and through the actions in the World Trade Organization \cite{EUWhitePaperAI2020Feb}.

\subsection{The current state and issues of data sovereignty \statusgreen }\label{sec:data-sovereignty}

As a consequence of the aforementioned Faustian bargain, the current model of the \gloss{World Wide Web} is flawed in more than one way. As a largely unforeseen consequence of economies of scale in infrastructure provision as well as network effects in social media, platforms became massive data silos, where vast amounts of user data goes through servers that belong to single organisations. This opened the possibility to collect, aggregate and analyse user data. 

The continued trend of replacing human-mediated interactions with computer-mediated interactions, as well as the rise of social media and the smartphone, have resulted in more and more information about our personal and social lives accessible to the companies behind these platforms. These have unraveled lucrative data markets where user demographics are linked with their underlying behavior, a treasure trove for marketing.



Data companies' business models gradually shifted towards capitalising on this asset to the point that nowadays their primary source of revenue is coming from selling the results of user profiling to advertisers and marketers. The circle is closed by showing advertisements to users on the same platforms, thus creating a feedback loop. A whole new industry grew out of this flow of information, and as a result, sophisticated systems emerged that predict and influence ways users allocate their attention and money, often knowingly exploiting human weaknesses in responding to stimuli. The situation is not far from what we could call mass manipulation, where only the most aware can truly exercise their freedom of choice and preserve their intrinsic preference regarding consumption of content or purchasing habits.

The fact that business revenue is coming from the demand for user data is also reflected in quality of service. The end users' needs became secondary to the needs of the "real" customers often leading to ever poorer user experience and quality of service. This is especially painful in the case of social platforms where the inertia due to network effect essentially constitutes user lock-in. It is imperative to correct these misaligned incentives. In other words, provide the same services to users, but without such unfortunate incentives resulting from the centralised data model.

Not having control over their data has serious consequences on the economic potential of the users. Some refer to this situation, somewhat dramatically, as \gloss{data slavery}. 
The current situation of keeping data in unconnected silos has various drawbacks: 

\begin{itemize}
    \item \emph{unequal opportunity} - Centralised entities increase inequality as their systems siphon away a disproportionate amount of profit from the actual creators of the value.
    \item \emph{lack of fault tolerance} - They are a single point of failure in terms of technical infrastructure.
    \item \emph{corruption} - The concentration of decision making power constitutes an easier target for social engineering, political pressure, or corruption.
    \item \emph{single attack target} - The concentration of large amounts of data under the same security system increases the potential reward for hackers. 
    \item \emph{lack of service continuity guarantees} - Service continuity is in the hands of the organisation, and is only indirectly incentivised by reputation. This introduces the risk of inadvertent termination of the service due to bankruptcy, or regulatory or legal action.
    \item \emph{censorship} - Centralised control of data access allows for, and in most cases eventually leads to, decreased freedom of expression.
    \item \emph{surveillance} - Data flowing through centrally owned infrastructure offers easier access to traffic analysis and other methods of unsolicited monitoring.
    \item \emph{manipulation} - Monopolisation of the display layer allows the data harvesters to have the power to manipulate opinions by choosing which data is presented, in what order and when, calling into question the sovereignty of individual decision making.
\end{itemize}


\subsection{Towards self-sovereign data \statusgreen} \label{sec:selfsovereigndata}

We believe that decentralisation is a major game-changer, which by itself solves a lot of the problems listed above.

We argue that blockchain technology is the final missing piece in the puzzle to realise the cypherpunk ideal of a truly self-sovereign Internet. One of the goals of this book is to demonstrate how decentralised consensus and peer-to-peer network technologies can be combined to form a rock-solid base-layer infrastructure. This foundation is not only resilient, fault tolerant, permissionless, and scalable, but also egalitarian and economically sustainable given a well designed system of incentives. Due to a low barrier of entry for participants, the adaptivity of these incentives ensures that prices automatically converge to the marginal cost.
Swarm is a \gloss{Web 3.0} stack that is decentralised, incentivised, and secured. In particular, the platform offers participants solutions for data storage, transfer, access, and authentication. These data services are more and more essential for economic interactions. By providing universal access to these services, with strong privacy guarantees and without borders or external restrictions, Swarm fosters the spirit of global voluntarism and represents the \emph{infrastructure for a self-sovereign digital society}.

\subsection{Artificial intelligence and self-sovereign data \statusgreen} \label{sec:AIdata}

Artificial Intelligence (AI) is promising to bring about major changes to our society. On the one hand, it is envisioned to allow for another level of magnitude of services offered and providing myriad of business opportunities, while on the other hand displacing many professions and jobs, when not merely augmenting them \cite{Lee2018Sep}.

The three "ingredients" needed for AI to do its work are: computing power, models and data. Today, computing power is readily available and specialized hardware is being developed to further facilitate processing. A large headhunt for AI talent has taken place in the previous decade and some companies have also managed to "silo" the specialized talent to work on the task to provide models and analysis. But, with today's state of the AI (deep learning) the greatest advantage can be gained by working on large sets of data.

The ones with access to sufficiently large amounts of data are often the organizations that have been working on systematically siloing it. This monopoly on data has allowed some multinationals to have unprecedented profits, while not sharing these proceeds appropriately with the people whose data they silo. Not only this, the data they withold has much more potential value for both individuals and the society than just, for example, determining which advertisement to show to the user. 

Perhaps it is not a coincidence, that major data and AI "superpowers" are emerging in the form of the USA and China (and the companies based that are based there), while the EU seems to be lagging behind, some could argue on the basis of too conservative policies around data. Warnings have been issued that an AI arms-race might already be in progress with a lot of countries being left behind as "data colonies" \cite{HarariDavos2020Mar}. Some even warn that as it currently stands, China and the United States will accumulate an insurmountable advantage as AI superpowers \cite{Lee2018Sep}.

But it need not be so. Decentralised technologies are a way to allow for the European values toward data to persevere and at the same time enable a fair data economy to gradually emerge. This is the direction that many consumer and tech organizations are pursuing, not only in Europe, but globally. The privacy issues are too great to be ignored by all but the most desensitised individuals. The examples set in Europe will be noticed and followed by others.

Self-sovereign storage might well be the only way that individuals can take back control of their data and privacy. Swarm offers at it's core solutions for many of the problems facing today's Internet and the distribution and storage of data. It is built for privacy from the ground up, with intricate encryption of data and completely secure and leak-proof communication. Furthermore, it enables sharing of selected data with 3rd parties, on the terms of the individual. Payments and incentives are integral parts of Swarm, making financial compensation   in return for granular sharing of data a core concern.

While data on individuals is currently gathered in huge quantities by companies, it remains completely unavailable and out of their control. For the individuals to have all their data at their disposal in Swarm would mean leveraging a fuller set of data and getting better services, while having the option to contribute it to the global good with self verifyable anonymisation. The best of both worlds.

This new wider availiability of data for students and up and coming startups with disruptive ideas working in the AI and big-data sectors would facilitate the evolution of the whole field. With the facilities that Swarm provides, a new set of options is will open up for the companies and services providers. With proper decentralisation of data, we can own the extremely large and valuable data sets  that needed to build state-of-the-art AI models collectively. And offer personalized services to individuals. The playing field will be level for all, not skewed towards those in sole possession of proprietary data sets. 




\subsection{Collective information \statusgreen}\label{sec:collective_information}

\glossupper{collective information} started to accumulate with the emergence of the Internet, yet the concept has just recently become recognized and discussed under a variety of headings like \emph{fair data} or \emph{information commons}.

A collective, as defined by Wikipedia (itself an example of "collective information") is:
\begin{displayquote}
"A group of entities that share or are motivated by at least one common issue or interest, or work together to achieve a common objective." 
\end{displayquote}
The internet allows collectives to be formed on a previously unthinkable scale, despite differences in geographic location, age, social status, and other demographics. Data,  produced by these collectives, through joint interaction on public forums, reviews, votes, and polls or metadata is a form of collective information. Since most of these are facilitated by for-profit entities, running centralized servers, the collective information ends up being stored in data silos owned by a single entity, the majority held by a few big technology companies.

These "platform economies" are becoming increasingly more important in a digitizing society. We are, however, seeing that the information they acquire over their users is increasingly being used against their interests. This calls into question whether these corporations are capable of bearing the ethical responsibility that comes with the power of holding \gloss{collective information} in mass.

State actors are also trying to get access to personal data, with some countries demanding to have a kind of back-door access to data always available to them. Here the European Union seems to be heading the other way by not only empowering individuals with rights to have access and export of their data, but also in allowing the individuals their rights to privacy. 
As AI has the potential for misuse or ethically questionable use, a number of EU countries have started ethical initiatives, regulations and certifications for AI use, for example the German Data Ethics Commission, Denmark's Data Ethics Seal and a voluntary certification in Malta \cite{EUWhitePaperAI2020Feb}. A EU wide ethical framework is planned. 

Yet, even if corporations would be capable of bearing such responsibility, the mere existence of \glossplural{data silo} stifles innovation. The client-server architecture is nascent to this problem, that made centralised data storage unavoidable (see \ref{sec:web_1} and \ref{sec:web_2}). Peer-to-peer networks (\ref{sec:peer_to_peer}) make it possible to change this architecture, hence, enabling collective ownership of collective information. 

Infrastructure is emerging to allow for governance structure, organisation and access to data taking into account the interests of all involved parties - producers and users of data \cite{DigitalPlurality2020Feb}. Different names are used for solutions that allow this data stewardship: data trusts, data co-operatives, personal data stores. Some see that a fair data economy, taking into account also the interests of citizens and consumers, will take off in the EU if proper trust can be attained in the continent's data infrastructure, reversing the laggard role EU has in the world today in the field of data. 

As the EU has identified the problem of not having enough data available for innovative re-use and AI applications, it is planning to boost the single market by establishing EU-wide data pools (also called data spaces) for data analysis and machine learning in strategic sectors. It will also seek to rebalance data infrastructure between centralised and highly distributed models,  with smart data processing at the edge \cite{EUDataStrategy2020Feb}. These data spaces will cover sectors of industry, mobility, health and others. 


\section{The vision  \statusorange}\label{sec:vision}

\wip{Gregor: still working on text}

\begin{displayquote}
Swarm is infrastructure for a self-sovereign society. 
\end{displayquote}


\subsection{Values \statusorange}\label{sec:values}

Self-sovereignty implies freedom. If we break it down, this implicates the following metavalues: 

\begin{itemize}
\item \emph{inclusivity} - Public and permissionless participation.  
\item \emph{integrity} - Privacy, provable provenance. 
\item \emph{incentivisation} - Alignment of interest of node and network.
\item \emph{impartiality} -  Content and value neutrality.  
\end{itemize}

These metavalues can be thought of as systemic qualities which contribute to empowering individuals and collectives in the direction of self-sovereignty.

Inclusivity entails we aspire to include the underprivileged in data economy; lower barrier of entry to define complex data flow and build decentralised applications. Swarm is a network with open participation for providing service and permissionless access to publishing, sharing, and investing your data.

Users are free to express their intention as action and have the authority to decide if they remain anonymous or share their interaction and preferences. The integrity of online persona is required. 

Economic incentives serve to make sure participants' behaviour align with the desired emergent behaviour of the network (see \ref{sec:incentivisation}). 

Impartiality basically says that catering for the other three values are not only necessary but sufficient: it rules out values that would treat any particular group as privileged or express preference for data with a particular content or from a particular source.

\subsection{Design principles \statusorange}\label{sec:design-principles}
 

Information society  and data economy bring an age where online transactions and big data are crucial to everyday life and thus the supporting technology counts as critical infrastructure. It is imperative that this base layer infrastructure be provided with strong guarantees for continuity. 

Continuity is achieved by the following \emph{systemic} requirements:

\begin{itemize}
\item \emph{stable} - The specifications and software implementations are stable and resilient to changes in participation, or politics (political pressure, censorship).
\item \emph{scalable} - The solution can accommodate many orders of magnitude more users and data than initially starting without a prohibitive hit on performance or reliability (mass adoption).
\item \emph{self-sustaining} - The solution runs by itself as an autonomous system, not depending on social coordination of collective action or any legal entity's business, exclusive know-how or infrastructure.   
\item \emph{secure} - The solution is resistant to deliberate attacks, immune to  social pressure  and politics and resilient in terms of tolerating errors in technologies it relies on. 
\end{itemize}





\subsection{Objectives \statusyellow}\label{sec:objectives}

%\subsubsection{Scope}

When we informally talk about the flow of data, we mean how information has  integrity across modalities, see table \ref{tab:scope}. This corresponds to the original Ethereum vision.

\begin{table}[htb]
\centering
\begin{tabular}{c|c|c}
dimension & model & project area\\\hline
%
time & memory & storage \\
space & messaging & communication \\
symbolic & manipulation & processing \\
\end{tabular}
\caption{Scope of the project and data integrity in 3 dimensions.}
\label{tab:scope}
\end{table}

Global infrastructure that supports data storage, transfer and processing 
can be thought of as the \gloss{world computer}. If the Ethereum blockchain is the CPU of the world computer, then Swarm is the "hard disk".

The Swarm project is set out to bring this vision to completion and build the remaining parts covering storage and communication. 

\subsection{Impact areas \statusorange}

In what follows, we try to identify feature areas of the product that best express or facilitate the values discussed above. 

Inclusivity in terms of permissionless participation is best guaranteed in a decentralised peer-to-peer network.  
Allowing nodes to provide service and get paid doing so could offer a zero-cash entry to the ecosystem: new users without money can serve other nodes until they accumulate enough to use services themselves. A decentralised network providing distributed storage without gatekeepers is also inclusive and impartial in that it allows content creators currently deplatformed to publish without being censored. 

Economic incentives built in the protocols is working best if it tracks the actions that incur costs in the context of peer to peer interaction: bandwidth sharing evidenced in message relaying is such an action where immediate accounting is possible if a node receives message valuable to them. On the other hand, promissory services such as commitment to preserve data over time, need to be rewarded only upon verifying. In order to avoid the tragedy of commons problem, such promissory commitments should be guarded by individual accountability through the threat of punitive measures, i.e., allow staked insurers.

Integrity is served by easy provability of authenticity, while potentially maintaining anonymity.
Provable inclusion and uniqueness are important to allow trustless data transformations.

% \subsection{Requirements \statusred}\label{sec:requirements}

\subsection{The future} \label{sec:future}

The future is unknown and many challenges lie ahead for humanity. What is certain in today's digital society is that to be sovereign and in control of our destinies, nations and individuals alike need to keep access and control over their data and communication.

Swarm's vision and objectives stem from the decentralized tech community and it's values, as it was originally designed to be the "hard drive" for the world computer that is Ethereum.

It will offer responsiveness suitable for dapps running real-time on the devices of users and properly incentivizing storage on any kind of storage backend - from laptop to high-availability clusters. This will be done with incentivization for storage and bandwidth done in a way guaranteeing service continuity guarantees.

Content creators will get fair compensation for the content they will offer and content consumers will be paying for it, removing the middlemen providers that currently benefit from the network effects. The benefits of the network effects will be spread throughout the network.

But it will be much more than that. Every individual, every device is leaving a trail of data. That data is picked up and stored in silos, its potential used up only in part and to the benefit of large players.

Swarm will be the go-to place for digital mirror worlds. Individuals, societies and nations will have a cloud storage solution, that will not depend on any one large provider. 

% This is especially important for countries currently lagging behind, such as Africa and Latin America. 

Individuals will be able to fully control their own data. They will no longer have the need to be part of the data slavery, giving their data in exchange for services. Not only that, they will be able to organize into data collectives or data co-operatives - sharing certain kinds of data as commons for reaching common goals. 

Nations will establish self-sovereign Swarm clouds as data spaces to cater to the emerging artificial intelligence industry - in industry, health, mobility and other sectors. The cloud will be between peers, though maybe inside an exclusive region, and third parties will not be able to interfere in the flow of data and communication - to monitor, censor,  or manipulate it. Yet, any party with proper permissions will be able to access it thus hopefully levelling the playing field for AI and services based on it.  

Swarm can, paradoxically, be the "central" place to store the data and enable robustness of accessibility, control, fair distribution of value and leveraging the data for benefiting individuals and society.

Swarm will become ubiquitious in the future society, transparently and securely serving data of individuals and devices to data consumers in the Fair data economy.

