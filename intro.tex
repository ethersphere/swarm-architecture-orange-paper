\chapter{The vision}\label{chap:vision}

 

\section{Motivation}

In order to both appreciate the problems we are trying to solve, a little history is useful.

While the Internet in general and the World Wide Web in particular dramatically reduced the costs of disseminating information, putting (almost) a publisher's power at (almost) every user's fingertips, these costs are still not zero and their allocation heavily influences who gets to publish what content and who gets to enjoy it.

\subsection{Web 1.0}

In the times of Web 1.0, in order to have your content accessible by the whole world, you would typically fire up a web server or use some web hosting (free or cheap) and upload your content that could be navigated through a set of html pages. If your content was unpopular, you still had to either maintain the server or pay the hosting to keep it accessible, but true disaster struck when, for one reason or another, it became popular (e.g. you got "slashdotted"). At this point, your traffic bill skyrocketed just before either your server crashed under the load or your hosting provider throttled your bandwidth to the point of making your content essentially unavailable for the majority of your audience. If you wanted to stay popular, you had to invest in high availability clusters and fat pipes and with the growth of your popularity, your costs grew, without any obvious way to cover them. There were very few practical ways to let (let alone \emph{make}) your audience share the ensuing financial burden directly.

The common wisdom at the time was that it would be the internet service providers that would come to the rescue, since in the early days of the Web revolution, bargaining about peering arrangements between ISP's involved arguments about where providers and where consumers are and which ISP is making money from the other's network. Indeed, when there was a sharp disbalance between originators of TCP connection requests (i.e. SYN packets), it was customary for the originator ISP to pay the recipient ISP, which made the latter somewhat incentivized to help those hosting popular content. In practice, however, this incentive structure usually resulted in putting a free \emph{pr0n} or \emph{warez} server in the server room to tilt the scales of SYN packet counters. Blogs catering to a niche audience had no way of competing and were generally left out in the cold. Note, however, that back then, creator-publishers typically owned their content.

\subsection{Web 2.0}

The transition to Web 2.0 changed much of that. Context-sensitive targeted advertising offered a Faustian bargain to content producers. As in "We give you scalable hosting that would cope with any traffic your audience throws at it, but you give us substantial control over your content; we are going to track each member of your audience and learn -- and own -- as much of their personal data as we can, we are going to pick who can and who cannot see it, we are going to proactively censor it and we may even report on you, for the same reason". As a consequence, millions of small content producers created immense value for very few corporations, getting only peanuts (typically, free hosting) in exchange.

\subsection{Peer-to-peer networks}

At the same time, however, the P2P revolution was gathering pace. Actually, P2P traffic very soon took over the majority of packets flowing through the pipes, quickly overtaking the above mentioned SYN-bait servers. If anything, it proved beyond doubt that using the hitherto massively underutilized upstream bandwidth of regular end-users, they could get the same kind of availability and bandwidth for their content as that provided by big corporations with data centers attached to the fattest pipes of the internet's backbone. What's more, this could be achieved at a fraction of the cost. In particular, users retained a lot more control and freedom over their data. Finally, this mode of data distribution proved to be remarkably resilient even in the face of powerful and well-funded entities extending great efforts to shut it down.


On the other hand, even the most evolved mode of P2P file sharing, which is trackerless Bittorrent, is just that: file sharing. It is not suitable for providing the kind of interactive, responsive experience that people came to expect from web applications on Web 2.0. Simply sharing upstream bandwidth and hard-drive space and a tiny amount of computing power without proper accounting and indexing only gets you so far. However, if you add to the mix a few more emergent technologies, most importantly, the blockchain, you get what we believe deserves the Web 3.0 moniker: a decentralised, censorship-resistant way of sharing and even collectively creating interactive content, while retaining full control over it. The price is surprisingly low and mostly consists of the resourses supplied by the super-computer (by yesteryear's standards) that you already own or can rent for cheap.

\subsection{The economics of bittorrent and its limits}

The genious of Bittorrent lies in its clever resource optimisation: If many clients want to download the same content from you, give them different parts of it and let them swap the missing parts between one another in a tit-for-tat fashion. This way, the upstream bandwidth use of a user hosting the content (\emph{seeder} in Bittorrent parlance) is roughly the same, no matter how many clients want to download it simultaneously. This solves the most painful issue of the ancient HTTP underpinning the World Wide Web.

Cheating (i.e. feeding your peers with garbage) is discouraged by the use of hierarchical piecewise hashing, whereby a package offered for download is identified by a single short hash, and any part can be cryptographically proven to be a specific part of the package without all the other parts, with a very small overhead. This beautifully simple approach has three main shortcomings, somewhat related:

\begin{itemize}
    \item \emph{lack of economic incentives} -- 
There are no built-in incentives to seed downloaded content. In particular, one cannot exchange the upstream bandwidth provided by seeding one content for downstream bandwidth required for downloading some other content. Effectively, upstream bandwidth provided by seeding somebody else's content is not directly rewarded in any way.
\item \emph{initial latency} -- 
 Typically, downloads start slowly and with delay. Clients that are further ahead in downloading have much more to offer to and much less to demand from newcomers. This results in bittorrent downloads starting as a trickle before turning into a full-blown torrent of bits. This severely limits the use of bittorrent in responsive interactive applications.
\item \emph{lack of content addressing} -- Small chunks of data can only be shared in the context of the larger file that they are part of. We find peers sharing the content we seek by querying the Distributed Hash Table (DHT) for said file. Thus a peer sharing only part of a file needs to know what that file is in order to be found in the DHT, and conversely, if the peer doesn't know that the data chunks belong to some file the peer will not be found by users seeking that file. This commonly happens for example when the same chunks of data appear verbatim in multiple files. Also, unless their objective is simply to get the missing parts of a file from their peers, nodes are not rewarded for their sharing efforts (storage and bandwidth), just like seeders.

\end{itemize}

\subsection{Towards Web 3.0}

The straightforward approach of using bittorrent as a distribution mechanism for web content has been successfully implemented by \gloss{Zeronet} \cite{zeronet}. However, because of the aforementioned issues with bittorrent, Zeronet fails to provide the same kind of responsive experience that users of web services came to expect. TRON \cite{tron} also implemented basic incentivisation on top of bittorrent.

In order to enable responsive distributed web applications (called \gloss{dapps} in Web 3.0 communities), IPFS \cite{ipfs2014} had to introduce a few major improvements over Bittorrent. The most immediately apparent novelty is the highly web-compatible URL-based retrieval. In addition, the directory (also organized as a DHT) has been vastly improved, making it possible to search for any part of any file (called \emph{chunk}). It has also been made very flexible and pluggable in order to work with any kind of storage backend, be it a laptop with intermittent wifi, or a sophisticated HA cluster in a fiber-optic connected datacenter.



Because of the improved directory, it is in the interest of greedy downloaders to balance the load they impose on other nodes; unlike in the case of bittorrent, they do not need to be forced to do so. The naive default behavior of IPFS nodes is to download what they want as fast as  they can from those who provide it, while automatically caching, advertising and uploading upon request everything they come across. They use their downstream bandwidth to the maximum extent they can, while do not limit the use of their upstream bandwidth beyond their physical limit. This, together with a few very powerful and well-connected nodes provided by the company behind IPFS, results in a very impressive performance even without any additional incentive module.


One measure by which IPFS aims to shield its users from legal liability is that, just like in the case of bittorrent, there is no such thing as "pushing" anything onto an IPFS node. Sharing anything on IPFS simply means making it available on one's own node and known in the directory. However, naive consumers immediately replicate all the content they download and also make it available. Public HTTP gateways (most run by the company behind IPFS) provide automatic replication for whatever content is being accessed through them.

The same problem with lack of incentives is apparent in various other projects such as zeronet or MAIDSAFE. Incentivization for distributed document storage is still a relatively new research field, especially in the context of the brand new blockchain technology. The Tor network has seen suggestions \cite{jansen2014onions,hoshetal2014tor} but these schemes are largely academic, they are not built in at the heart of the underlying system. Bitcoin has also been repurposed to drive other systems like Permacoin \cite{miller2014permacoin} or Sia \cite{vorick2014sia}, some use their own blockchain, altcoin  such  as Filecoin \cite{filecoin2014} for IPFS.

What is still missing from the above incentive system, is the possibility to rent out large amounts of disk space to those willing to pay for it, irrespective of the popularity of their content; and conversely there is also a way to deploy your interactive dynamic content to be stored in the cloud, a feature we call \emph{upload and disappear}.

The objective of any incentive system for p2p content distribution is to encourage cooperative behavior and discourage freeriding: the uncompensated depletion of limited resources. The incentive strategy outlined here aspires to satisfy the following constraints:

\begin{itemize}
    \item  It is in the node's interest irrespective of whether other nodes follow it or not.
    \item It makes it expensive to hog other nodes' resources.
    \item It does not impose unreasonable overhead.
    \item It plays nice with "naive" nodes.
    \item It rewards those that play nice, including those following this strategy.
\end{itemize}

In the context of swarm, storage and bandwidth are the two most important limited resources and this is reflected in our incentive scheme. The incentives for bandwidth use are designed to achieve speedy and reliable data provision while the storage incentives are designed to ensure long term data preservation, ideally solving the "upload and disappear" problem.

\subsection{Data  sovereinty}

As a consequence of aforementioned Faustian bargain, the current model of the world wide web is flawed in more than one way.
 As a largely unforeseen consequence of economy of scale in infrastructure provision as well as network effect in social media, platforms became massive data silos, where vast amounts of user data goes through servers that belong to single organisations. This opened the possibility to collect, aggregate and analyse user data. 
On the one hand, more and more economic interactions are online. On the other hand, with the rise of social media, more and more information about our personal and social lives is accessible to the companies behind these platforms. These trends led to opportunities to link demographic information with economically relevant user behaviours, a treasure trove for marketing.
Data companies business models gradually shifted towards capitalising on this asset in as much as their primary source of revenue is coming from reselling linked datasets gathered through user profiling to advertisers and marketeers. The circle is closed by showing advertisements to users on the same platforms creating a feedback loop. A whole new industry grew out of this flow of information and as  a result, sophisticated systems emerged that predict, influence ways users allocate their attention and money, often knowingly exploiting human weaknesses in responding to news and stimulus. The situation is not far from what we could call mass manipulation where only the most aware can truely exercise their freedom of choice and preserve their intrinsic preference regarding consumption of content or purchasing habits.
The fact that business revenue is coming from the demand for user data is also reflected in quality of service. The end users needs became secondary to the needs of the "real" customers often leading to ever poorer user experience and quality of service. This is especially painful in the case of social platforms where the inertia due to the network effect essentially constitutes user lock-in. It is imperative to correct these misaligned incentives in other words provide the same services to users but without the hidden incentive resulting from centralised data  access.

The fact that users have no control over their data has serious consequences on their economic potential, a situation  referred to by some, somewhat dramatically, as \emph{data slavery}. 
The current situation of data silos have various drawbacks: 

\begin{itemize}
    \item \emph{equal opportunity} - centralised entities increase inequality as the system siphons disproportionate profit away from creators of the value
    \item \emph{fault tolerance} - constitute single points of failure in terms of technical infrastructure 
    \item \emph{corruption} - concentration of decision making power and ownership makes an easier targer for social engeneering, political pressure, or corruption 
    \item \emph{attack resilience} - concentration of large amounts of data under the same security system increases potential reward for hackers attacks is massive), 
    \item \emph{continuity} - service continuity is virtually in the hands of the organisation and only indirectly incentivised by reputation; inadvertent termination due to bankruptcy or regulatory or legal action is a risk factor    
    \item \emph{censorship} - centralised control of data access allows for and in most cases eventually leads to decreased freedom of expression
    \item \emph{surveillance} - data flowing through centrally owned infrastructure offers easier access to traffic analysis and other methods of unsolicited monitoring even if anonimity and confidentiality are technologically warranted.
\end{itemize}

Web 3.0 has on its banner the objective to rectify this and provide infrastructure for a self sovereign society. We believe that decentralisation is a major game changer which by itself solves most of the problems listed above.

we argue that blockchain technology is the final missing piece in the puzzle to realise the cypherpunk ideal of a truly sovereign internet. Our goal with this book is to show that decentralised consensus and peer to peer network technologies are ripe for a  marriage and their wholesome union will give birth to a rock solid base layer infrastructure that is not only resilient, fault tolerant, permissionless and scalable but also economically sustainable due to engineered adaptive incentives. Given the context of low barrier of entry for participant, adaptivity entails that prices are automatically driven to converge on marginal cost.
swarm is a web3 stack that is decentralised incentivised and secured. In particular, the platform offers solutions to data storage, transfer, access, authentication  for participants: data services that are central to more and more of economic interactions. By providing permissionless access to these services with strong privacy guarantees without borders and external restrictions,
swarm forters the spirit of global voluntarism.









\section{Requirements}


\section{Design principles}
% \chapter{Related work}

