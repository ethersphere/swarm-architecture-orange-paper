\chapter{The evolution  \statusgreen}\label{chap:vision}



The rest of this chapter gives background information to the evolution leading to Swarm and finally the vision of Swarm. \ref{sec:historical_context} lays out a historical analysis of the World Wide Web, focusing on how it became the place it is today.
\ref{sec:fair-data} explains the importance of data sovereignty, collective information and \gloss{fair data economy}. It discusses why a self-sovereign society needs infrastructure to collectively host, move and process data.
Finally, \ref{sec:vision} recaps the values behind the vision, spells out the requirements of the technology and lays down the design principles that guide us in manifesting Swarm.

\section{Historical context  \statusgreen}\label{sec:historical_context}
\green{}
While the Internet in general and the \gloss{World Wide Web} (\gloss{WWW}) in particular dramatically reduced the costs of disseminating information, putting a publisher's power at every user's fingertips, the costs of disseminating this data are still not zero and their allocation heavily influences who gets to publish what content and who will consume it.

In order to appreciate the problems we are trying to solve, a little journey into the history of the evolution of the \gloss{World Wide Web} is useful.

\subsection{Web 1.0 \statusgreen}\label{sec:web_1}

In the times of \gloss{Web 1.0}, in order to have your content accessible to the whole world, you would typically fire up a web server or use some free or cheap web hosting to upload your content that could then be navigated through a set of HTML pages. If your content was unpopular, you still had to either maintain the server or pay the hosting to keep it accessible, but true disaster struck when, for one reason or another, it became popular (e.g. you got "Slashdotted"). At this point, your traffic bill skyrocketed just before either your server crashed under the load or your hosting provider throttled your bandwidth to the point of making your content essentially unavailable for the majority of your audience. If you wanted to stay popular, you had to invest in high availability clusters connected with fat pipes; your costs grew together with your popularity, without any obvious way to cover them. There were very few practical ways to allow (let alone require) your audience to share the ensuing financial burden directly.

The common wisdom at the time was that it would be the \gloss{ISP} that would come to the rescue, since in the early days of the Web revolution bargaining about peering arrangements between ISP's involved arguments about where providers and consumers are located, and which ISP is making money from the other's network. Indeed, when there was a sharp imbalance between originators of TCP connection requests (aka SYN packets), it was customary for the originator ISP to pay the recipient ISP, which made the latter somewhat incentivised to help support those hosting popular content. In practice, however, this incentive structure usually resulted in putting a free \emph{pr0n} or \emph{warez} server in the server room to tilt back the scales of SYN packet counters. Blogs catering to a niche audience had no way of competing and were generally left out in the cold. Note, however, that back then, creator-publishers still typically owned their content.

\subsection{Web 2.0 \statusgreen}\label{sec:web_2}

The transition to \gloss{Web 2.0} changed much of that. A transition from a personal home page running on one's own server, using Tim Berners Lee's elegantly simplistic and accessible hypertext markup language toward server-side scripting using cgi-gateways, perl and the inexorable php caused a divergence from the beautiful idea that anyone could write and run their own website using simple tools. This set the web on a path towards a prohibitively difficult and increasingly complex stack of scripting languages and databases. Suddenly the world wide web wasn't such a beginner friendly place to be any more, and at the same time new technologies began to make it possible to create web applications which could provide simple user interfaces to enable publishers to simply POST their data to the server, and divorce themselves of the responsibilities of actually delivering the bits to their end users. In this way, the Web 2.0 was born.

Capturing the initial maker spirit of the web, now sites like MySpace and Geocities ruled the web. These sites offered average users a piece of the internet to call their own, complete with as many scrolling text marquees, flashing pink glitter Comic Sans banners and ingenious XSS attacks a script kiddie could dream of. It was a web within the web, an accessible and open environment for users to start publishing their own content without rules. Platforms abounded and suddenly there was a place for everyone, a phpBB forum for any niche interest imaginable and suddenly the web became full of life and the dotcom boom showered Silicon Valley in riches.

However, this youthful naivete could not last and the fabulous rainbow coloured playground couldn't last. The notoriously unreliable MySpace fell victim to its open policy of allowing scripting. Users' pages became unreliable and the platform became unusable. When Facebook arrived with a clean looking interface that worked, MySpace had no chance and people migrated in droves. The popular internet suddenly got serious, and we filed into the stark white corporate office of Facebook. But there was trouble in store, while offering this service for free, Mr. Zuckerberg and others were keeping a secret. In return for hosting our data, we would have to trust them with it (dumb f*cks), and we did. For the time being there was no business model, simply get more financing, amass huge user-bases and deal with the problem later. Extensive and unreadable T\&C's gave all the rights to the content to the platforms. Now nothing was guaranteed, while in Web 1.0 it was easy to keep a backup of your website and migrate to a new host, or simply host it from home yourself, now those who had controversial views had a new verb to deal with: 'deplatformed'.

At the infrastructure level, this centralisation began to manifest itself in unthinkably huge data-centers. Jeff Bezos pivoted his book-selling business to become the richest man on earth by enabling those unable to deal with the technical and financial hurdles of implementing increasing complex and expensive infrastructure. Now there was a middle of the road solution to deal with those irregular traffic spikes. Others followed and soon huge amounts of the web began to be hosted by a few huge companies. Corporate acquisitions and endless streams of VC caused more and more concentration of power, Google provided paradigm shifting ways to organise and access this exponential proliferation of data, and helping to depose the stalwart Microsoft's attempt to force the web into a hellish existence, forever imprisoned in Internet Explorer 6, but could finally no longer keep it's promise to 'do no evil' and instead succumbed to it's own megalomania and began to eat the competition. Steadily email became Gmail and Google crept into every facet of daily life in the web.

On the surface, everything was rosy. Technological utopia was making the world hyper-connected in a way no-one could have imagined. No longer was the web just for academics and the super 1337, it made the sum of human knowledge available to anyone, and now that smartphones became ubiquitous, it could be accessed anywhere. Wikipedia, gave everyone superhuman knowledge, Google allowed us to find and access it in a moment and Facebook gave us the ability to communicate with everyone we'd ever known, for free. However, underneath all this, there were problems buried just below this shiny glass facade.

Finally it was time to cash the cheque of unlimited streams of capital. Now that the behemoth platforms had all the users, the time to work out a business model had come. In search of a way to provide value back to the shareholders, all the content providing platforms looked to advertising revenue to bring back to profit hungry shareholders. Now the web started to get complicated, and distracting. Advertising appeared everywhere and the pink flashing glitter banners were back, this time pulling your attention from the content you came for to deliver you to the next user acquisition opportunity.

If this weren't enough, there were more horrors to come. Truly the Web began to lose its innocence. As the proliferation of data became unwieldy, algorithms were provided to help better provide us access to the content that we want. Now the platforms had all our data, they were able to analyse this, and work out what we wanted to see even better than we knew ourselves. Now everyone would be fed their most favourite snack, along with the products they would most like to buy. But there was a catch to these secret algorithms and all encompassing data-sets: they were for sale to the highest bidder. Powerful and deep-pocketed political organisations were able to target swing voters with unprecedented accuracy and efficacy. Suddenly cyberspace had become very real, but consensus normality was a thing of the past, and now fake news ruled the airwaves.

At the same time, more terrifying revelations lay in wait. It turned out the egalitarian ideals that had driven the initial construction of a trustful internet were the most naive of all. Edward Snowden walked out the NSA with a stack of documents no-one could have expected. It turned out the protocols were broken, and all the warrant canaries were dead -- the world's governments had been running a surveillance dragnet on the entire population -- storing, processing, cataloguing, indexing and providing access to the sum total of a persons online activity. It was all available at the touch of an XKeyStore button, an all seeing Optic Nerve determined to 'collect it all' and 'know it all' no matter who or what the context. 
This type of gross erosion of privacy, preceded by many other similar noted efforts by variously evil institutions and individuals across the world to track and block the packets of suppressed people, such as political adversaries or journalists targeted by repressive regimes, had provided impetus for the Tor project. This unusual collaboration between the US Military, MIT and the EFF had responded by providing not only a way to obfuscate the origin of a request but also to serve up content in a totally anonymous way. Although wildly successful and a household name, it has not found much use outside this specific niche use, due to relatively high latency as a result of it's inherent inefficiencies. 

By the time of the revelations of Snowden, the web had become ubiquitous and completely integral to almost every facet of human life, but the vast majority of it was run by corporations. While reliability problems became a thing of the past, there was a price to pay. Context-sensitive targeted advertising driven models now offered a Faustian bargain to content producers. "We will give you scalable hosting that will cope with any traffic your audience throws at it, but in return you must give us control over your content: we are going to track each member of your audience and collect -- and own -- as much of their personal data as we are able, we will decide who can and who cannot see it, we will proactively censor it, and willingly share your data with authorities where requested". As a consequence, millions of small content producers created immense value for very few corporations, getting only peanuts (typically, free hosting) in exchange.

Putting aside the seemingly inherent FUD of the Web 2.0 data apocalypse, there are also natural problems with the architecture. The corporate approach has engendered a centralist maxim, so all requests must be routed through the backbone, to a monolith data-center, passed around, then finally returned back, even if messaging someone in the next room. Client-server architecture with afterthought security is flimsy at best and often breached leading to oil-slicks of unencrypted personal data and even plaintext passwords spread all over the web. Spaghetti code implementations of growing complexity subdivide into multifarious micro-services. Even those with deep pockets find it difficult to deal with the development bills and fledgling start-ups drowning in a feature-factory sea of spiralling technical debt is common. A modern web application's current stack in all cases is a cobbled together Heath-Robinson machine comprising so many moving parts that it is almost impossible even for a supra-nation-state corporation to maintain and develop these implementations without numerous bugs and regular security flaws. It is time for a consolidation. 


\subsection{Peer-to-peer networks \statusgreen}\label{sec:peer_to_peer}

At the same time as the centrist Web 2.0 took over the world, quietly evolving in parallel, the \gloss{peer-to-peer} (\gloss{P2P}) revolution was also gathering pace. Actually, P2P traffic had very soon took over the majority of packets flowing through the pipes, quickly overtaking the above mentioned SYN-bait servers. If anything, it proved beyond doubt that end-users, by working together to use their hitherto massively underutilised upstream bandwidth, could achieve the same kind of availability and bandwidth for their content as previously provided only by big corporations with data centers attached to the fattest pipes of the Internet's backbone. What's more, this could be achieved at a fraction of the cost. In particular, users retained a lot more control and freedom over their data and finally, this mode of data distribution proved to be remarkably resilient even in the face of powerful and well-funded entities exerting great efforts to shut it down.

However, even the most evolved mode of \gloss{P2P} file sharing, tracker-less Bittorrent \cite{pouwelse2005bittorrent}, was only that: file level sharing. It was not at all suitable for providing the kind of interactive, responsive experience that people came to expect from web applications on \gloss{Web 2.0}. In addition to this, although proving to be an extremely popular, BitTorrent was never conceived of with economics or game theory in mind, and before the world could possibly have known the coming revolution it's namesake would precipitate.

\subsection{The economics of BitTorrent and its limits \statusgreen}

The genius of \gloss{BitTorrent} lies in its clever resource optimisation \cite{cohen2003incentives}: if many clients want to download the same content from you, give them different parts of it and let them swap the missing parts between one another in a tit-for-tat fashion. This way, the upstream bandwidth use of a user hosting the content (the \gloss{seeder} in BitTorrent parlance) is roughly the same, no matter how many clients want to download it simultaneously. This solves the most painful issue of the ancient \gloss{HTTP}, the protocol underpinning the \gloss{World Wide Web}.

Cheating (i.e. feeding your peers with garbage) is discouraged by the use of hierarchical piecewise hashing, whereby a package offered for download is identified by a single short hash, and any part of it can be cryptographically proven to be a specific part of the package without all the other parts, with a very small computational overhead. 

This beautifully simple approach has three main shortcomings \cite{locher2006free,piatek2007incentives}, all somewhat related.

\begin{itemize}
\item \emph{lack of economic incentives} -- 
There are no built-in incentives to seed downloaded content. In particular, one cannot exchange his upstream bandwidth provided by seeding one's content, for the downstream bandwidth required for downloading some other content. Effectively, the upstream bandwidth provided by seeding content to another user is not directly rewarded in any way.
\item \emph{initial latency} -- 
 Typically, downloads start slowly and with some delay. Clients that are further ahead in downloading have much more to offer to newcomers, but much less to demand from them. The result of this is that BitTorrent downloads start as a trickle before turning into a full-blown torrent of bits. This peculiarity severely limits the use of BitTorrent in interactive applications that require both fast responses and high bandwidth.
\item \emph{lack of content addressing} -- Small \glossplural{chunk} of data can only be shared in the context of the larger file that they are part of. Peers are found by sharing the content that is seeked by querying the \gloss{distributed hash table} (\gloss{DHT}) for the said file. In other words, the advertising of the available content happens at the level of files, as opposed to the chunks of data they are composed of. This leads to inefficiencies as the same chunks of data often appear verbatim in multiple files. 
\item \emph{no incentive to keep sharing} --
Nodes are not rewarded for their sharing efforts (storage and bandwidth) once they have achieved their objective of getting the missing parts of a file from their peers.
\item \emph{no privacy or ambiguity} --
Nodes advertise exactly the content they are seeding. It is easy for attackers to discover the IP address of peers hosting content they would like to see remove, and then a simple step for adversaries such as nation states to petition the ISP for the physical location of the connection. This has led to a grey market of VPN providers helping users circumvent this. Although these services offer assurances of privacy, it is in reality impossible to verify them as the implementations are necessarily closed source.
\end{itemize}

Although it has become a spectacularly popular and very useful, BitTorrent is at the same time primitive. One can only get so far by simply sharing upstream bandwidth, hard-drive space, and a tiny amount of computing power, without proper accounting and indexing. However, if we add to the mix a few more emergent technologies, most importantly, the blockchain, we get something we believe truly deserves the \gloss{Web 3.0} moniker: a decentralised, censorship-resistant way of sharing and even collectively creating interactive content, all while retaining full control over it. What's more, the cost of this can consist almost entirely of the resources supplied by the super-computer (by yesteryear's standards) that you already own.

\subsection{Towards Web 3.0 \statusgreen}\label{sec:towards-web3}

% 0/ intro talk about the limitations and problems with web2 app architecture 
% 1/ why the game has changed in a post-satoshi world
% As the blockchain has brought us the ability to 
% 2/ why swarm represents a further iteration on this change and makes the whole thing usable, how it overcomes limitations of the blockchain, emphasis the VC problem, talk about making the web fun again
% 3/ some short exploration of current attempts to provide this and their potential limitations, but keep this short, unemotional and unbiased
% 4/ drum up to grand ending of how swarm will provide trustless computing save the world etc. etc.

\begin{displayquote}
The Times 03/
Jan/2009 Chancel
lor on brink of 
second bailout f
or banks
\end{displayquote}

At 6:15 on (or after) Saturday the 3rd January 2009 the world changed forever. A mysterious cypherpunk created the first block of a chain that would come to encircle the entire world, and the genie was officially out of the bottle. This first step would put in motion a set of reactions that would result in an unprecedentedly enormous amount of money flowing from the traditional reservoirs of fiat and physical goods into a totally new way to store and transmit value -- cryptocurrency -- Satoshi Nakamato did something no-one else had been able to do, he disintermediated the banks, decentralised trustless value transfer and in that moment money became owned by everyone.

This first step was a huge one and it was to be a monumental turning point, however, as much as it was conceptually brilliant, it had problems with utility. We could transmit digital value, we could even colour the coins or transmit short messages like the one above included to mark the fateful date of the first block, but that's it. Every transaction must be stored on every node, 100 bytes maximum extra data, sharding not included.

When Vitalik conceived of Ethereum, the utility of the system took a massive leap. Including the facility for turing-complete computation via the EVM revealed a horizon of applications that they themselves would run in this trustless setting. The concept was as brilliant and it was paradigm shifting. Now we had authentication and value transfer baked into the language at its very core and it again changed everything. The possibilities were numerous and alluring and Web 3.0 was born. However, there was still a problem to overcome when transcending fully from the Web 2.0 world: storing data on the blockchain was prohibitively expensive for anything but a tiny amount.

The straightforward approach of using \gloss{BitTorrent} as a distribution mechanism for web content had been successfully implemented by \gloss{ZeroNet} \cite{zeronet}. However, because of the aforementioned issues with BitTorrent, ZeroNet is unable to provide the same kind of responsive experience that users of web services have come to expect. 

In order to try to enable responsive \glossplural{distributed web application} (a \glossplural{dapp}), the \gloss{InterPlanetary File System} (\gloss{IPFS}) \cite{ipfs2014} has introduced a few major improvements over BitTorrent. The most immediately apparent one is highly web-compatible URL-based retrieval. In addition, the directory and indexing of data, (also organized as a \gloss{DHT}) had been vastly improved, making it possible to search for a small part of any file, called a \gloss{chunk}.

There are also numerous other efforts to fill the gap of providing a Web 3.0 surrogate for the constellation of servers and services that have come to be expected by a Web 2.0 developer. These are not insignificant roles to supplant, even the most simple web app still subsumes a vast array of concepts and paradigms which have to be remapped into this now trustless setting. In many ways, this problem is proving to be perhaps even more nuanced that implementing trustless computation in the blockchain. Swarm responds to this with an array of carefully designed data structures, which enable the application developer to recreate concepts we have grown used to in Web 2.0 into the new setting of Web 3.0. Later we will describe how it is possible in Swarm to reimagine the current state of the web, but this time built on solid cryptographic foundations.

A sliding scale can be perceived with large file size, low frequency of retrieval and a more monolithic api on the left, and smaller data packets, higher frequency of retrieval and more nuanced api on the right. In this graph, file storage and retrieval systems like a posix filesystem, S3, Storj and BitTorrent live on the left hand side, whereas key-value stores like LevelDB or databases like MongoDB or Postgres live on the right. Across this scale, all these modalities are needed to build a useful app, and furthermore there must be the ability to combine data where necessary, and to ensure only authorised parties have access to their data. In a centrist model it is easy to handle these problems initially, more difficult with growth, but every part of the scale has a solution from one piece of software or another. However, in the decentralised model, all bets are off the table. Authorisation must be handled with cryptography and the combination of data is hence limited by this. As a result, the Web 3.0 stack is nascent and evolving, and many solutions are only deal piecemeal with only part of this spectrum of requirements. In this book, you will learn how Swarm spans each area of this spectrum, as well as providing high level tools for the new guard of budding Web 3.0 developers. And so we hope from an infrastructure perspective, working on Web 3.0 will feel like the halcyon days of Web 1.0, while delivering unprecedented levels of security and privacy.

To respond to the need for a privacy to be baked in at the root level in file-sharing, as it is so effectively in Ethereum, Swarm insists on anonymity at a very fundamental and absolute level. Lessons from Web 2.0 have taught us that trust should be given away carefully and only to those that are deserving of it and will treat it with respect. Data is toxic, and we must treat it carefully in order to be responsible to ourselves and those for whom we take responsibility. Later we will explain how Swarm provides full and fundamental user privacy.

And of course, to fully transition to a Web 3.0 decentralised world, we must also deal with the problems of incentive and trust that are traditionally solved by handing over responsibility to the (often untrustworthy) centralised gatekeeper. As we have noted, this is one problem that BitTorrent has also struggled to solve and responded to with a plethora of seed ratios and private (centralised) trackers.

The problem as framed as the lack of financial incentive to host and store content is also apparent in various other projects such as ZeroNet or MaidSafe. Incentivisation for distributed document storage is still a relatively new research field, especially in the context of the brand new blockchain technology. The Tor network has seen suggestions \cite{jansen2014onions,ghoshetal2014tor} but these schemes are largely academic, they are not built-in at the heart of the underlying system. Bitcoin has also been repurposed to drive other systems like Permacoin \cite{miller2014permacoin}, some use their own blockchain, such as Sia \cite{vorick2014sia} or Filecoin \cite{filecoin2014} for IPFS. BitTorrent is also currently testing the waters with blockchain incentivisation with help of their own token \cite{tron2018,bittorrent2019}. However, even combined together, there would still be many hurdles to overcome to provide the specific requirements for a Web 3.0 dapp developer.

We will see later how Swarm provides a full suite of incentivisation measures, as well as other checks and balances to ensure that nodes are working to benefit the whole of the swarm. This includes the possibility to rent out large amounts of disk space to those willing to pay for it, irrespective of the popularity of their content; all while ensuring there is also a way to deploy your interactive dynamic content to be stored in the cloud, a feature we call \gloss{upload and disappear}.

The objective of any incentive system for \gloss{P2P} content distribution is to encourage cooperative behavior and discourage \gloss{freeriding}: the uncompensated depletion of limited resources. The \gloss{incentive strategy} outlined here aspires to satisfy the following constraints:

\begin{itemize}
    \item It is in the node's own interest, regardless of whether other nodes follow it.
    \item It must be expensive to expend the resources of other nodes.
    \item It does not impose unreasonable overhead.
    \item It plays nice with "naive" nodes.
    \item It rewards those that play nice, including those following this strategy.
\end{itemize}

In the context of Swarm, storage and bandwidth are the two most important limited resources and this is reflected in our incentives scheme. The incentives for bandwidth use are designed to achieve speedy and reliable data provision, while the storage incentives are designed to ensure long term data preservation. In this way, we ensure that all requirements of web application development are not only provided for, but that incentives are provided so that each individual node's actions benefit not only itself, but the whole of the network. 

\section{Fair data economy  \statusgreen}\label{sec:fair-data}
\green{}

In the era of \gloss{Web 3.0}, the Internet is no longer just a niche where geeks play, but has become a fundamental conduit of value creation and growing economic activity. Yet the data economy in it's current state could not be labelled as fair, as the distribution of the spoils is under the control of those who control the data - mostly companies keeping this data to themselves in isolated 'data silos'. In order for us to acheive the goal of a \gloss{fair data economy} many social, legal and technological issues will have to be tackled. We will now describe some of the problems that currently present and how they will be addressed by Swarm. 

\subsection{The current state of the data economy  \statusgreen} \label{sec:dataeconomy}

Digital mirror worlds already exist, those that contain shadows of physical things and consist of unimaginably large amounts of data \cite{MirrorWorlds2020Feb}. Yet more and more data will continue to be synced to these parallel worlds, requiring new infrastructure and markets, and creating new business opportunities. Only relatively crude measures exist for measuring the size of the data economy as a whole, but for the USA, one figure puts the financial value of data (with related software and intellectual property) at \$1.4trn-2trn in 2019 \cite{MirrorWorlds2020Feb}. The EU Commission projects the figures for the data economy in the EU27 for 2025 at €829bln (up from €301bln in 2018) \cite{EUDataStrategy2020Feb}. 

Despite this huge amount of value, the asymmetric distribution of the wealth generated by the existing data economy has been put forward as a major humanitarian issue \cite{TheWinner2020Feb}. As efficiency and productivity will rise, as a results of better data, the profits that result from this will need to be distributed. Today, the spoils are distributed unequally: the larger the companies' data set, the more it can learn from the data, attract more users and hence even more data. Currently, this is most apparent with the dominating large tech companies such as FAANG, but it is predicted that this will also be increasingly important in non-tech sectors and even nation states. Hence, companies are racing to become dominant in a particular sector. And countries hosting these platforms will gain an advantage. As Africa and Latin America host so few of these, they risk becoming exporters of raw data and then paying other countries to import the intelligence provided, as has been warned by the United Nations Conference on Trade and Development \cite{TheWinner2020Feb}. Another problem is that if a large company monopolizes a data market, it could also become the sole purchaser of "data labour" - maintaining a complete control of setting prices and affording the possiblity that "data wages" could be manipulated to keep them artificially low. In many ways, we are already seeing evidence of this. 

% move this?
% As a solution, citizens could organise into "data co-operatives", who would then act as trade unions do in conventional economy. 

Flows of data are becoming increasingly blocked and filtered by governments using reasoning based on the protection of citizens, sovereignty and national economy \cite{VirtualNationalism2020Feb}. Leaks revealed by several security experts have shown that for governments to properly give consideration to national security,  data should be kept close to home and not left to reside in other countries. GDPR is such one instance of a "digital border" that has been erected -- data may leave the EU only if appropriate safeguards are in place. Other countries, such as India, Russia and China, have implemented their own geographic limitations on data. The EU Commission has pledged to closely monitor the policies of these countries and address any limits or restrictions to data flows in trade negotiations and through the actions in the World Trade Organization \cite{EUWhitePaperAI2020Feb}.

Despite this growing interest in the ebb and flow of data, it is the big tech firms that firmly hold a grip on much of this, and the devil is in the details. Swarm's privacy first model requires that no personal data be divulged to any third parties, so everything is end-to-end encrypted out of the box, resisting the ability of service providers to aggregate and leverage giant datasets. The outcome of this is that instead of being concentrated at the service provider, control of the data remains decentralised and with the individual to which it pertains, and as a result, so does the power.

\subsection{The current state and issues of data sovereignty \statusgreen }\label{sec:data-sovereignty}

As a consequence of the aforementioned Faustian bargain, the current model of the \gloss{World Wide Web} is flawed in more than one way. As a largely unforeseen consequence of economies of scale in infrastructure provision, as well as network effects in social media, platforms became massive data silos, where vast amounts of user data passes through, and is held on servers that belong to single organisations. This opened the possibility to collect, aggregate and analyse user data. 

The continued trend of replacing human-mediated interactions with computer-mediated interactions, as well as the rise of social media and the smartphone, have resulted in more and more information about our personal and social lives accessible to the companies behind these platforms. These have unraveled lucrative data markets where user demographics are linked with their underlying behavior, a treasure trove for marketeers.

Data companies' business models gradually shifted towards capitalising on this asset to the point that nowadays their primary source of revenue is coming from selling the results of user profiling to advertisers, marketeers and others who would seek to influence members of the public. The circle is closed by showing advertisements to users on the same platforms, thus creating a feedback loop. A whole new industry has grown out of this flow of information, and as a result, sophisticated systems emerged that predict, guide and influence ways for users allocate their attention and money, often knowingly exploiting human weaknesses in responding to stimuli and even resorting to highly developed and calculated psychological manipulation. The situation is not far from what we could call mass manipulation, where not even the most aware can truly exercise their freedom of choice and preserve their intrinsic autonomy of preference in consumption of content or purchasing habits.

The fact that business revenue is coming from the demand for micro-targeted users to advertise to is also reflected in quality of service. The end users' needs became secondary to the needs of the "real" customers, often leading to ever poorer user experience and quality of service. This is especially painful in the case of social platforms where the inertia due to network effect essentially constitutes user lock-in. It is imperative to correct these misaligned incentives. In other words, provide the same services to users, but without such unfortunate incentives resulting from the centralised data model.

Not having control over their data has serious consequences on the economic potential of the users. Some refer to this situation, somewhat dramatically, as \gloss{data slavery}. 
The current situation of keeping data in disconnected silos has various drawbacks: 

\begin{itemize}
    \item \emph{unequal opportunity} - Centralised entities increase inequality as their systems siphon away a disproportionate amount of profit from the actual creators of the value.
    \item \emph{lack of fault tolerance} - They are a single point of failure in terms of technical infrastructure, notably security.
    \item \emph{corruption} - The concentration of decision making power constitutes an easier target for social engineering, political pressure, or corruption.
    \item \emph{single attack target} - The concentration of large amounts of data under the same security system increases the potential reward for hackers. 
    \item \emph{lack of service continuity guarantees} - Service continuity is in the hands of the organisation, and is only indirectly incentivised by reputation. This introduces the risk of inadvertent termination of the service due to bankruptcy, or regulatory or legal action.
    \item \emph{censorship} - Centralised control of data access allows for, and in most cases eventually leads to, decreased freedom of expression.
    \item \emph{surveillance} - Data flowing through centrally owned infrastructure offers easier access to traffic analysis and other methods of unsolicited monitoring.
    \item \emph{manipulation} - Monopolisation of the display layer allows the data harvesters to have the power to manipulate opinions by choosing which data is presented, in what order and when, calling into question the sovereignty of individual decision making.
\end{itemize}


\subsection{Towards self-sovereign data \statusgreen} \label{sec:selfsovereigndata}

We believe that decentralisation is a major game-changer, which by itself solves a lot of the problems listed above.

We argue that blockchain technology is the final missing piece in the puzzle to realise the cypherpunk ideal of a truly self-sovereign Internet. One of the goals of this book is to demonstrate how decentralised consensus and peer-to-peer network technologies can be combined to form a rock-solid base-layer infrastructure. This foundation is not only resilient, fault tolerant, permissionless, and scalable, but also egalitarian and economically sustainable, with a well designed system of incentives. Due to a low barrier of entry for participants, the adaptivity of these incentives ensures that prices automatically converge to the marginal cost.
Swarm is a \gloss{Web 3.0} stack that is decentralised, incentivised, and secured. In particular, the platform offers participants solutions for data storage, transfer, access, and authentication. These data services are more and more essential for economic interactions. By providing universal access to all for these services, with strong privacy guarantees and without borders or external restrictions, Swarm fosters the spirit of global voluntarism and represents the \emph{infrastructure for a self-sovereign digital society}.

\subsection{Artificial intelligence and self-sovereign data \statusgreen} \label{sec:AIdata}

Artificial Intelligence (AI) is promising to bring about major changes to our society. On the one hand, it is envisioned to allow for a myriad of business opportunities, while on the other hand it expected to displace many professions and jobs, when not merely augmenting them \cite{Lee2018Sep}.

The three "ingredients" needed for AI to do its work are: computing power, models and data. Today, computing power is readily available and specialized hardware is being developed to further facilitate processing. An extensive headhunt for AI talent has been taking place for more than a decade and companies have also managed to monopolise workers in possesion of the specialised talents needed to work on the task to provide the models and analysis. However, with today's state of the AI (deep learning) the greatest advantage of all can be gained by working on the largest sets of data.

Those with access to sufficiently large amounts of data are often the organizations that have been systematically siloing it, often by providing the user with an application with some utility, such as social media, and then stockpiling the data for uses other than those exposed to the users without their express consent for this purpose. This monopoly on data has allowed some multinationals to make unprecedented profits, with no concern to sharing these financial proceeds appropriately with the people whose data they have sold. Not only this, the data they withhold is prevented from fulfilling its potentially massive value not only for individuals but for society as a whole.

Perhaps it is not a coincidence, therefore, that the major data and AI "superpowers" are emerging in the form of the governments of the USA and China and the companies based that are based there. Warnings have been issued that an AI arms-race might already be in progress with a lot of countries being left behind as "data colonies" \cite{HarariDavos2020Mar}. Some even warn that as it currently stands, China and the United States will inevitably accumulate an insurmountable advantage as AI superpowers \cite{Lee2018Sep}.

But we believe it need not be so. Decentralised technologies and cryptography are a way to allow for privacy of data, and at the same time enable a fair data economy to gradually emerge. This is the direction that many consumer and tech organizations are pursuing across the globe in order to be able to compete with the data behemoths, with many users beginning to realise that they are being swindled into giving away their data to big tech. Swarm will provide the infrastructure for to facilitate this choice.

And self-sovereign storage might well be the only way that individuals can take back control of their data and privacy as the first step toward reclaiming the autonomy of their own culture. Swarm represents at its core solutions for many of the problems facing today's Internet and the distribution and storage of data. It is built for privacy from the ground up, with intricate encryption of data and completely secure and leak-proof communication. Furthermore, it enables sharing of selected data with 3rd parties, on the terms of the individual. Payments and incentives are integral parts of Swarm, making financial compensation in return for granular sharing of data a core concern.

While data on individuals is currently gathered in huge quantities by companies, it remains completely unavailable and out of their control. For the individuals to have access to all their own data through the swarm, would mean the ability to leverage a fuller set of data and demand better services, while still having the option to contribute it to the global good with self-verifiable anonymisation. The best of all worlds.

This new wider availability of data for young academic students and up and coming startups with disruptive ideas working in the AI and big-data sectors would facilitate the evolution of the whole field. With the facilities that Swarm provides, a new set of options will open up for the companies and services providers. With widespread decentralisation of data, we can collectively own the extremely large and valuable data sets that are needed to build state-of-the-art AI models. The portability of this data, already a trend that is being hinted at in traditional tech will enable competition and even personalised services for individuals. The playing field will be levelled for all. 




\subsection{Collective information \statusgreen}\label{sec:collective_information}

\glossupper{collective information} already began to accumulate from the first emergence of the Internet, yet the concept has just recently become recognized and discussed under a variety of headings such as  \emph{open source}, \emph{fair data} or \emph{information commons}.

A collective, as defined by Wikipedia (itself an example of "collective information") is:
\begin{displayquote}
"A group of entities that share or are motivated by at least one common issue or interest, or work together to achieve a common objective." 
\end{displayquote}
The internet allows collectives to be formed on a previously unthinkable scale, despite differences in geographic location, age, social status, and other demographics. Data,  produced by these collectives, through joint interaction on public forums, reviews, votes, and polls or metadata is a form of collective information. Since most of these are facilitated by for-profit entities, running centralized servers, the collective information ends up being stored in data silos owned by a single entity, the majority held by a few big technology companies.

These "platform economies" are becoming increasingly more important in a digitising society. We are, however, seeing that the information they acquire over their users is increasingly being used against their interests. This calls into question whether or not these corporations are capable of bearing the ethical responsibility that comes with the power of holding \gloss{collective information} in mass.

While many state actors are trying to obtain unfettered access to the collective mass of personal data of individuals, with some countries demanding magic-key like back-door access, there are exceptions. Since AI has the potential for misuse or ethically questionable use, a number of countries have started ethical initiatives, regulations and certifications for AI use, for example the German Data Ethics Commission or Denmark's Data Ethics Seal. 

Yet, even if corporations could be capable of bearing such responsibility, the mere existence of \glossplural{data silo} stifles innovation. The client-server architecture is nascent to this problem, as it has made centralised data storage the default (see \ref{sec:web_1} and \ref{sec:web_2}). Effective peer-to-peer networks such as Swarm (\ref{sec:peer_to_peer}) now make it possible to alter the topology of this architecture, hence enabling the collective ownership of collective information. 


\section{The vision  \statusorange}\label{sec:vision}

\wip{Gregor: still working on text}

\begin{displayquote}
Swarm is infrastructure for a self-sovereign society. 
\end{displayquote}


\subsection{Values \statusorange}\label{sec:values}

Self-sovereignty implies freedom. If we break it down, this implies the following metavalues: 

\begin{itemize}
\item \emph{inclusivity} - Public and permissionless participation.  
\item \emph{integrity} - Privacy, provable provenance. 
\item \emph{incentivisation} - Alignment of interest of node and network.
\item \emph{impartiality} -  Content and value neutrality.  
\end{itemize}

These metavalues can be thought of as systemic qualities which contribute to empowering individuals and collectives in the direction of self-sovereignty.

Inclusivity entails we aspire to include the underprivileged in the data economy; lower barrier of entry to define complex data flow and build decentralised applications. Swarm is a network with open participation for providing service and permissionless access to publishing, sharing, and investing your data.

Users are free to express their intention as action and have the authority to decide if they remain anonymous or share their interaction and preferences. The integrity of online persona is required. 

Economic incentives serve to make sure participants' behaviour align with the desired emergent behaviour of the network (see \ref{sec:incentivisation}). 

Impartiality basically says that catering for the other three values are not only necessary but sufficient: it rules out values that would treat any particular group as privileged or express preference for data with a particular content or from a particular source.

\subsection{Design principles \statusorange}\label{sec:design-principles}
 

Information society and the data economy bring an age where online transactions and big data are crucial to everyday life and thus the supporting technology is critical infrastructure. It is imperative that this base layer infrastructure be provided with strong guarantees for continuity. 

Continuity is achieved by the following \emph{systemic} requirements:

\begin{itemize}
\item \emph{stable} - The specifications and software implementations are stable and resilient to changes in participation, or politics (political pressure, censorship).
\item \emph{scalable} - The solution can accommodate many orders of magnitude more users and data than initially starting with, without experiencing prohibitively high reductions in performance or reliability during mass adoption.
\item \emph{self-sustaining} - The solution runs by itself as an autonomous system, not depending on social coordination of collective action or any legal entity's business, exclusive know-how or infrastructure.   
\item \emph{secure} - The solution is resistant to deliberate attacks, immune to  social pressure and politics and resilient in tolerating faults in its technological dependencies (e.g. blockchain, programming languages). 
\end{itemize}
)



\subsection{Objectives \statusyellow}\label{sec:objectives}

%\subsubsection{Scope}

When we informally talk about the flow of data, we mean how information has  integrity across modalities, see table \ref{tab:scope}. This corresponds to the original Ethereum vision.

\begin{table}[htb]
\centering
\begin{tabular}{c|c|c}
dimension & model & project area\\\hline
%
time & memory & storage \\
space & messaging & communication \\
symbolic & manipulation & processing \\
\end{tabular}
\caption{Scope of the project and data integrity in 3 dimensions.}
\label{tab:scope}
\end{table}

A global infrastructure that supports data storage, transfer and processing 
can be thought of using a loose analogy as the \gloss{world computer}. Then, if the Ethereum blockchain is the CPU of the world computer, then Swarm is best thought of as the "hard disk". Of course, as is inherent, this model belies the complex nature of Swarm, and Swarm is capable of much more than simple storage, as we will discuss later in the book.

The Swarm project is set out to bring this vision to completion and build the remaining parts covering storage and communication. 

\subsection{Impact areas \statusorange}

In what follows, we try to identify feature areas of the product that best express or facilitate the values discussed above. 

Inclusivity in terms of permissionless participation is best guaranteed by a decentralised peer-to-peer network.  
Allowing nodes to provide service and get paid for doing so will offer a zero-cash entry to the ecosystem: new users without currency can serve other nodes until they accumulate enough currency to use services themselves. A decentralised network providing distributed storage without gatekeepers is also inclusive and impartial in that it allows content creators who risk being deplatformed by repressive and unjust authorities to publish without their right to free speech being censored. 

Economic incentives built in the protocols works best if it tracks the actions that incur costs in the context of peer-to-peer interactions: bandwidth sharing as evidenced in message relaying is one such action where immediate accounting is possible as a node receives a message that is valuable to them. On the other hand, promissory services such the commitment to preserve data over time, must be rewarded only upon verifying. In order to avoid the \gloss{tragedy of commons} problem, such promissory commitments should be guarded against by enforcing individual accountability through the threat of punitive measures, i.e. by allowing staked insurers.

Integrity is served by easy provability of authenticity, while maintaining anonymity.
Provable inclusion and uniqueness are fundamental to allowing trustless data transformations.

% \subsection{Requirements \statusred}\label{sec:requirements}

\subsection{The future} \label{sec:future}

The future is unknown and many challenges lie ahead for humanity. What is certain in today's digital society is that to be sovereign and in control of our destinies, nations and individuals alike must retain access and control over their data and communication.

Swarm's vision and objectives stem from the decentralized tech community and its values, as it was originally designed to be the "hard drive" component in the trinity which would form the world computer: Ethereum, Whisper, and Swarm.

It offers the responsiveness required by dapps running on the devices of users and well incentivised storage using any kind of storage infrastructure - from smartphone to high-availability clusters. Continuity will be guaranteed with well designed incentives for bandwidth and storage.

Content creators will get fair compensation for the content they will offer and content consumers will be paying for it, removing the middlemen providers that currently benefit from the network effects. The benefits of the network effects will be spread throughout the network.

But it will be much more than that. Every individual, every device is leaving a trail of data. That data is picked up and stored in silos, its potential used up only in part and to the benefit of large players.

Swarm will be the go-to place for digital mirror worlds. Individuals, societies and nations will have a cloud storage solution, that will not depend on any one large provider. 

% This is especially important for countries currently lagging behind, such as Africa and Latin America. 

Individuals will be able to fully control their own data. They will no longer have the need to be part of the data slavery, giving their data in exchange for services. Not only that, they will be able to organize into data collectives or data co-operatives - sharing certain kinds of data as commons for reaching common goals. 

Nations will establish self-sovereign Swarm clouds as data spaces to cater to the emerging artificial intelligence industry - in industry, health, mobility and other sectors. The cloud will be between peers, though maybe inside an exclusive region, and third parties will not be able to interfere in the flow of data and communication - to monitor, censor,  or manipulate it. Yet, any party with proper permissions will be able to access it thus hopefully levelling the playing field for AI and services based on it.  

Swarm can, paradoxically, be the "central" place to store the data and enable robustness of accessibility, control, fair distribution of value and leveraging the data for benefiting individuals and society.

Swarm will become ubiquitious in the future society, transparently and securely serving data of individuals and devices to data consumers in the Fair data economy.

